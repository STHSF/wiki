<!DOCTYPE HTML>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <title>Spark的存储不同格式文件 - LiYu's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Economics"/>
    <meta name="description" content="A wiki website of sthsf when I learned new knowledgy and technics."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-78529611-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>
<div id="container">
    
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#Spark">Spark</a>&nbsp;»&nbsp;Spark的存储不同格式文件</div>
</div>
<div class="clearfix"></div>
<div id="title">Spark的存储不同格式文件</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">写在前面</a><ul>
<li><a href="#spark">启动 Spark （如果你已经启动就不需要）</a></li>
<li><a href="#1csv">1、存储为csv格式</a></li>
<li><a href="#2">2、将文档保存在一个文件夹中</a></li>
<li><a href="#3json">3、存储为json格式</a></li>
<li><a href="#jsoncsv">注意：其中json的内存要比csv大（存储空间）</a></li>
<li><a href="#4parquet">4、存储为parquet格式</a><ul>
<li><a href="#_2">列式存储</a></li>
<li><a href="#parquet">parquet常用操作</a><ul>
<li><a href="#parquet_1">创建parquet表</a></li>
<li><a href="#parquet_2">读取parquet文件</a></li>
</ul>
</li>
<li><a href="#flinkkafkaparquethdfssparkparquet">Flink读取kafka数据并以parquet格式写入HDFS，Spark直接读取parquet</a></li>
<li><a href="#fastparquet">fastparquet</a></li>
</ul>
</li>
<li><a href="#5compression-">5、存储为compression格式---压缩</a></li>
<li><a href="#6table">6、存储为table</a></li>
<li><a href="#_3">参考文献</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">写在前面</h1>
<p><strong><em>PySpark的存储不同格式文件，如：存储为csv格式、json格式、parquet格式、compression格式、table</em></strong></p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span><span class="p">,</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
</pre></div>


<h2 id="spark">启动 Spark （如果你已经启动就不需要）</h2>
<div class="hlcode"><pre><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>
</pre></div>


<h2 id="1csv">1、存储为csv格式</h2>
<div class="hlcode"><pre><span class="n">df_csv</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s">&quot;../data/ratings.csv&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_csv</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s">&#39;../output/rating.csv&#39;</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="s">&#39;error&#39;</span><span class="p">)</span> <span class="c">#保存数据</span>
</pre></div>


<h2 id="2">2、将文档保存在一个文件夹中</h2>
<div class="hlcode"><pre><span class="sx">!ls -lh ../output/rating.csv  #根据数量保存多个文件</span>

<span class="sx">!head ../output/rating.csv/part-00001-aece805c-20a7-4225-b152-40316bc8fc5e-c000.csv</span>
</pre></div>


<div class="hlcode"><pre><span class="n">df_csv</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s">&#39;../output/rating2.csv&#39;</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="hlcode"><pre><span class="sx">!ls -lh ../output/rating.csv</span>
</pre></div>


<h2 id="3json">3、存储为json格式</h2>
<div class="hlcode"><pre><span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s">&#39;../output/rating.json&#39;</span><span class="p">,</span><span class="n">mode</span> <span class="o">=</span> <span class="s">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>


<div class="hlcode"><pre><span class="sx">!ls -lh ../output/rating.json   #根据数量保存多个文件</span>
</pre></div>


<h2 id="jsoncsv">注意：其中json的内存要比csv大（存储空间）</h2>
<h2 id="4parquet">4、存储为parquet格式</h2>
<div class="hlcode"><pre><span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&#39;../output/rating.parquet&#39;</span><span class="p">,</span><span class="n">mode</span> <span class="o">=</span> <span class="s">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>


<div class="hlcode"><pre><span class="sx">!ls -lh ../output/rating.parquet  #根据数量保存多个文件</span>
</pre></div>


<h3 id="_2">列式存储</h3>
<p>列式存储和行式存储相比有哪些优势呢？</p>
<p>1、可以跳过不符合条件的数据，只读取需要的数据，降低 IO 数据量。</p>
<p>2、压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，<br />
可以使用更高效的压缩编码（例如 Run Length Encoding 和 Delta Encoding）进一步节约存储空间。</p>
<p>3、只读取需要的列，支持向量运算，能够获取更好的扫描性能。</p>
<h3 id="parquet">parquet常用操作</h3>
<h4 id="parquet_1">创建parquet表</h4>
<p>1.1 创建内部表</p>
<p>1.2 创建外部表</p>
<p>1.3 指定压缩算法</p>
<h4 id="parquet_2">读取parquet文件</h4>
<p>用spark写parquet文件</p>
<div class="hlcode"><pre><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkConf</span><span class="o">().</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;test&quot;</span><span class="o">).</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sqlContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">SQLContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>

<span class="c1">// 读取文件生成RDD</span>
<span class="k">val</span> <span class="n">file</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;hdfs://192.168.1.115:9000/test/user.txt&quot;</span><span class="o">)</span>

 <span class="c1">// 定义parquet的schema，数据字段和数据类型需要和hive表中的字段和数据类型相同，否则hive表无法解析</span>
<span class="k">val</span> <span class="n">schema</span> <span class="k">=</span> <span class="o">(</span><span class="k">new</span> <span class="nc">StructType</span><span class="o">)</span>
      <span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">,</span> <span class="nc">StringType</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
      <span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="s">&quot;age&quot;</span><span class="o">,</span> <span class="nc">IntegerType</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="n">rowRDD</span> <span class="k">=</span> <span class="n">file</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\t&quot;</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">Row</span><span class="o">(</span><span class="n">p</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="nc">Integer</span><span class="o">.</span><span class="n">valueOf</span><span class="o">(</span><span class="n">p</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">trim</span><span class="o">)))</span>
<span class="c1">// 将RDD装换成DataFrame</span>
<span class="k">val</span> <span class="n">peopleDataFrame</span> <span class="k">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="o">(</span><span class="n">rowRDD</span><span class="o">,</span> <span class="n">schema</span><span class="o">)</span>
<span class="n">peopleDataFrame</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">&quot;people&quot;</span><span class="o">)</span>
    <span class="n">peopleDataFrame</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="o">(</span><span class="s">&quot;hdfs://192.168.1.115:9000/user/hive/warehouse/test_parquet/&quot;</span><span class="o">)</span>
</pre></div>


<p>用pyspark读取parquet文件</p>
<div class="hlcode"><pre><span class="c"># encoding:utf-8</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span><span class="p">,</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">HiveContext</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="c"># Basic Data Configuration</span>
<span class="n">APP_NAME</span> <span class="o">=</span> <span class="s">&quot;DataClean1_getHDFSparquetFile&quot;</span>
<span class="n">parquetFile</span> <span class="o">=</span> <span class="s">&quot;hdfs://192.168.136.134:9000/user/hadoop/people2.parquet&quot;</span>  <span class="c"># Which parquetFile to read</span>
<span class="n">sparkURL</span> <span class="o">=</span> <span class="s">&quot;spark://192.168.136.134:7077&quot;</span>
<span class="n">HADOOP_USER_NAME</span> <span class="o">=</span> <span class="s">&quot;hadoop&quot;</span>

<span class="c"># Test Configuration</span>
<span class="n">jsonFile</span> <span class="o">=</span> <span class="s">&quot;hdfs://192.168.136.134:9000/user/hadoop/people.json&quot;</span>


<span class="k">def</span> <span class="nf">read_parquet</span><span class="p">(</span><span class="n">sc1</span><span class="p">):</span>
    <span class="n">hive_ctx</span> <span class="o">=</span> <span class="n">HiveContext</span><span class="p">(</span><span class="n">sc1</span><span class="p">)</span>
    <span class="c"># Python中的Parquet数据读取</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">hive_ctx</span><span class="o">.</span><span class="n">parquetFile</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
    <span class="n">names</span> <span class="o">=</span> <span class="n">rows</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">ages</span> <span class="o">=</span> <span class="n">rows</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">age</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;Everyone&quot;</span>
    <span class="k">print</span> <span class="n">names</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">ages</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">row_collect</span> <span class="o">=</span> <span class="n">rows</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">row_collect</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">line</span>

    <span class="c"># 数据查询</span>
    <span class="n">rows</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
    <span class="n">peoples</span> <span class="o">=</span> <span class="n">hive_ctx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT name,age FROM people WHERE age&gt;24&quot;</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;people:&quot;</span>

    <span class="k">def</span> <span class="nf">function1</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">row</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">&quot;_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">age</span><span class="p">)</span>

    <span class="n">list1</span> <span class="o">=</span> <span class="n">peoples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">function1</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">list1</span><span class="p">:</span>
        <span class="k">print</span> <span class="n">line</span>
    <span class="k">print</span> <span class="s">&quot;End of the file&quot;</span>


<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">save_test</span><span class="p">(</span><span class="n">sparkconf</span><span class="p">):</span>
    <span class="n">maxnum</span> <span class="o">=</span> <span class="mi">2147483647</span> <span class="o">/</span> <span class="mi">10000</span>
    <span class="c"># count = sparkconf.parallelize(xrange(0, maxnum)).map(sample).reduce(lambda a, b: a + b)</span>
    <span class="c"># print &quot;Pi is roughly %f&quot; % (4.0 * count / maxnum)</span>

    <span class="c"># 打开hive</span>
    <span class="k">print</span> <span class="s">&quot;Start Hive Context&quot;</span>
    <span class="n">hive_ctx</span> <span class="o">=</span> <span class="n">HiveContext</span><span class="p">(</span><span class="n">sparkconf</span><span class="p">)</span>
    <span class="c"># 基本查询示例</span>

    <span class="n">input1</span> <span class="o">=</span> <span class="n">hive_ctx</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">)</span>
    <span class="n">input1</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
    <span class="n">top_tweets</span> <span class="o">=</span> <span class="n">hive_ctx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT name,age FROM people&quot;</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;top_tweets ==&quot;</span><span class="p">,</span> <span class="n">top_tweets</span>
    <span class="k">print</span> <span class="s">&quot;type(top_tweets) ==&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">top_tweets</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;top_tweets.map ==&quot;</span><span class="p">,</span> <span class="n">top_tweets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_tweets</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">)</span>
    <span class="n">top_tweets</span><span class="o">.</span><span class="n">saveAsParquetFile</span><span class="p">(</span><span class="s">&quot;hdfs://192.168.136.134:9000/user/hadoop/people2.parquet&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">row</span><span class="o">.</span><span class="n">__str__</span><span class="p">()</span>
    <span class="c"># Configure Spark</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">APP_NAME</span><span class="p">)</span>
    <span class="c"># conf = conf.set(&quot;spark.executor.memory&quot;, &quot;512m&quot;)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">sparkURL</span><span class="p">)</span>
    <span class="n">conf</span> <span class="o">=</span> <span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;HADOOP_USER_NAME&quot;</span><span class="p">,</span> <span class="n">HADOOP_USER_NAME</span><span class="p">)</span>
    <span class="c"># print conf.getAll()</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">addPyFile</span><span class="p">(</span><span class="s">&quot;hdfsToSparkClean.py&quot;</span><span class="p">)</span>

    <span class="c"># Execute Main functionality</span>
    <span class="n">save_test</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
    <span class="n">read_parquet</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>


<h3 id="flinkkafkaparquethdfssparkparquet">Flink读取kafka数据并以parquet格式写入HDFS，Spark直接读取parquet</h3>
<p>大数据业务场景中，经常有一种场景：外部数据发送到kafka中，flink作为中间件消费kafka数据并进行业务处理；处理完成之后的数据可能还需要写入到数据库或者文件系统中，比如写入hdfs中；目前基于spark进行计算比较主流，需要读取hdfs上的数据，可以通过读取parquet.</p>
<h3 id="fastparquet">fastparquet</h3>
<p><a href="https://pypi.org/project/fastparquet/">fastparquet</a></p>
<h2 id="5compression-">5、存储为compression格式---压缩</h2>
<div class="hlcode"><pre><span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s">&#39;../output/rating_gzip.csv&#39;</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">compression</span> <span class="o">=</span> <span class="s">&#39;gzip&#39;</span><span class="p">)</span>
</pre></div>


<div class="hlcode"><pre><span class="sx">!ls -lh ../output/rating_gzip.csv  #根据数量保存多个文件</span>
</pre></div>


<h2 id="6table">6、存储为table</h2>
<div class="hlcode"><pre><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&#39;show tables&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s">&#39;rating_csv&#39;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from ratings_csv&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<h2 id="_3">参考文献</h2>
<p><a href="https://blog.csdn.net/xingxing1839381/article/details/81273351">PySpark的存储不同格式文件</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1356771">parquet常用操作</a></p>
<p><a href="http://blog.sina.com.cn/s/blog_4b1452dd0102x2af.html">PySpark取hdfs中parquet数据</a></p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2018-06-02 00:00 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: 'Spark的存储不同格式文件',
  owner: 'sthsf',
  repo: 'wiki',
  oauth: {
    client_id: '086c54c5fd95adfdc372',
    client_secret: '2ad9ebe87b952d2c77fccf99c334881b91eaa73d',
  },
  // ...
  // For more available options, check out the documentation below
})
gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

</div>
<div id="footer">
            <span>
                Copyright © 2019 LiYu.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/sthsf/wiki" target="_blank"> github </a>.
            </span>
</div>


<!--百度统计-->
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?90e1dcdd1938573c19f9ff6521188e91";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


</body>
</html>