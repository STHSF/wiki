<!DOCTYPE HTML>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <title>概率统计基本概念 - LiYu's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Economics"/>
    <meta name="description" content="A wiki website of sthsf when I learned new knowledgy and technics."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-78529611-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>
<div id="container">
    
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm">Algorithm</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm-math">math</a>&nbsp;»&nbsp;概率统计基本概念</div>
</div>
<div class="clearfix"></div>
<div id="title">概率统计基本概念</div>
<div id="content">
  <h1 id="_1">写在前面</h1>
<p>机器学习离不开概率统计和线性代数的基本知识，如果想透彻的理解某个机器学习算法必然绕不开各种数学公式，掌握一些基本概念能够让我们更快的理解和推导这些公式。在这里，我将我自己在学习过程中遇到的基本知识整理起来。</p>
<h1 id="_2">随机变量</h1>
<div class="hlcode"><pre><span class="err">如果微积分研究的是变量的数学</span><span class="p">,</span> <span class="err">那么概率论与数理统计研究的就是随机变量的数学</span><span class="p">.</span>
                                                    <span class="o">----</span><span class="err">贾俊平</span>
</pre></div>


<p>要弄清这两个概念, 得先从随机变量说起, 随机变量分为离散型随机变量和连续型随机变量, 不赘述, 了解到随机变量之后我们再了解下面几个概念.</p>
<h1 id="_3">离散型随机变量的概率分布, 概率函数和分布函数</h1>
<p>在理解概率分布函数和概率密度函数之前, 我们可以先了解一下概率分布和概率函数.</p>
<p>所谓概率函数, 就是用函数的形式来表达概率.<br />
$$<br />
P_i = P(x=a_i), (i=1,2,3,4.....)<br />
$$<br />
在这个函数里, 自变量$(x)$为随机变量的取值, 因变量$(P_i)$为自变量取某一值的概率.</p>
<p>概率分布即为概率的分布, 下面的表格被称为离散型随机变量的<strong>概率分布</strong>, 完整的表达应该是<strong>离散型随机变量的值分布和值的概率分布列表</strong>, 这样更容易理解, </p>
<p>X|$(x_1)$|$(x_2)$|.....|$(x_n)$|.....|<br />
-|-|-|-|-|-|-|-|-|<br />
P|$(P_1)$|$(P_2)$|.....|$(P_n)$|.....|</p>
<p>在理解离散型概率分布的时候, 尤其要注意一点, 就是随机变量X的取值一定是全部取值.</p>
<p>分布函数的完整名字是概率分布函数, 概率分布函数是概率函数的累加,我们先看下面一段定义:</p>
<p>设离散型随机变量$(X)$的分布律是$(P{X=X_k}=P_k, k=1,2,3,....)$</p>
<p>则,     $(F(x) = P(X&lt;=x) = \sum_{X_k&lt;x}P_k)$</p>
<p>由于$(F(x))$是X取$(&lt;=x)$的诸值$x_k$的概率之和, 故又称$(F(x))$为累积概率函数.</p>
<p>从上面的定义可以看出, </p>
<h1 id="_4">连续型随机变量的概率密度函数和概率分布函数</h1>
<h2 id="_5">随机变量的分布函数</h2>
<p>设 $(X(w))$ 是一个随机变量, 称函数 $(F(x)=P{x&lt;=x}, -\infty&lt;x&lt;\infty)$ 为随机变量 $(X)$ 的分布函数</p>
<h3 id="_6">分布函数的性质:</h3>
<ul>
<li>任意$(a&lt;b)$, 总有$(F(a)&lt;=F(b))$. (单调非减性)</li>
<li>$(F(x))$是一个右连续函数;</li>
<li>任意$(x\in R)$, 总有$0&lt;=F(x)&lt;=1$(有界性), 且$(\lim_{x\rightarrow-\infty}F(x)=0, \lim_{x\rightarrow\infty} F(x)=1)$</li>
</ul>
<h2 id="_7">概率密度函数</h2>
<p>如存在非负可积函数$f(x)$, 使随机变量X取值与任一区间$((a, b])$的概率可表示成:<br />
$$<br />
P(a&lt;X &lt;= b)=\int_{a}^bf(x)dx<br />
$$<br />
则称X为连续型随机变量, $f(x)$为X的概率密度函数, 简称概率密度函数或密度.</p>
<h3 id="_8">概率密度函数的性质</h3>
<ul>
<li>$(f(x)&gt;=0)$</li>
<li>$(\int_{-\infty}^{\infty}f(x)dx = 1)$</li>
</ul>
<h2 id="_9">分布函数和概率密度函数的关系</h2>
<ul>
<li>$(F(x) = \int_{-\infty}^{\infty}f(t)dt)$</li>
<li>$(f(x) = F'(x))$</li>
</ul>
<h2 id="_10">注意点</h2>
<ul>
<li>概率密度函数$(f(x))$在点a处的取值, 不等于事件$({X=a})$的概率. 但是该值越大, X在a点附近取值的概率越大.</li>
</ul>
<h1 id="_11">先验概率、后验概率、条件概率</h1>
<h2 id="_12">先验概率</h2>
<p>在贝叶斯统计中，先验概率分布，即关于某个变量X的概率分布，是在获得某些信息或者依据前，对X的不确定性所进行的猜测，这是对<strong>不确定性</strong>(而不是随机性)赋予一个量化的数值的表征，这个量化的数值可以是一个参数，或者是一个潜在的变量。</p>
<p>先验概率是指可以通过以往的经验和分析得到的概率，仅仅通过主观上的经验估计，也就是事先根据已有的只是推断。它是一种预判概率，可以是基于历史数据的统计，可以是由背景常识得出，也可以是人的主观观点给出，通常都是单独时间概率。</p>
<p><strong>先验概率的分类：</strong><br />
利用过去历史资料计算得到的先验概率，称为客观先验概率。<br />
当前历史资料无从取得或者资料不全时，凭人们的主观经验来判断得到的先验概率，称为主观先验概率。</p>
<h2 id="_13">条件概率</h2>
<p>一般指一个事件发生后另一个事件发生的概率，一般的形式为$(P(A|B))$表示B发生的条件下A发生的概率。</p>
<h2 id="_14">似然函数</h2>
<p>似然函数也成似然，是一个关于统计模型参数的函数，也就是这个函数中自变量是统计模型的参数，对于观测结果X，在参数集合$(\theta)$上的似然，就是在给定这些参数值的基础上，观察到的结果的概率$(L(\theta)=P(x|\theta))$。也就是说，似然是关于参数的函数，在参数给定的条件下，对于观察到的X的值的分布。</p>
<h4 id="_15">定义</h4>
<p>给定输出x时，关于参数$(\theta)$的似然函数$(L(\theta|x))$(在数值上)等于给定参数$(\theta)$后变量X=x的概率：</p>
<p>$$L(\theta)=P(x|\theta)$$</p>
<p>解释：<br />
对参数$(\theta)$的似然函数的求值，（在数值上）等于观测结果x在给定参数$(\theta)$下的条件概率，也就是x的后验概率。一般似然函数的值越大表明在结果X=x下，此参数$(\theta)$越合理。因此形式上，似然函数也是一种条件概率函数，但是我们关注的变量改变了，关注的是A取值为参数$(\theta)$的似然值。</p>
<p>似然函数在统计判断中有重大作用，如在最大似然估计和费雪信息之中的应用扽等。似然性和或然性以及概率的意思相近，都是指某种事件发生的可能性，但是在统计学中，这三个名词的含义又有明确的区分，。</p>
<p>概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果。</p>
<p>似然性则是用于在已知观测所得到的结果时，对有关事物的性质的参数进行估计。</p>
<h2 id="_16">贝叶斯定理</h2>
<p>贝叶斯公式，用来描述两个条件概率（后验概率之间的关系），比如$(P(A|B))$和$(P(B|A))$。按照乘法法则：</p>
<p>$$<br />
P(AB) = P(A)<em>P(B|A) = P(B)</em> P(A|B)<br />
$$</p>
<p>因此，公式也可以改写为：</p>
<p>$$<br />
P(A|B) = P(A)*P(B|A)/ P(B)<br />
$$</p>
<p>其中， $(P(B))$为标准化常量</p>
<p>贝叶斯法则表述如下：</p>
<p>$$<br />
P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^n{P(B|A_i)P(A_i)}}<br />
$$</p>
<p>其中 $(A_1, A_2, ..., A_n)$为完备事件组，即$(\bigcup_{i=1}^n{A_i}=\omiga)$,$(A_iA_j=\phi)$,$(P(A_i)&gt;0)$</p>
<h2 id="_17">后验概率</h2>
<p>后验概率可以通过贝叶斯定理，用先验概率和似然函数计算出来，下面的公式就是用先验概率乘上似然函数，接着进行归一化，得到不定量X在Y=y的条件下的密度，即后验概率密度。</p>
<h2 id="_18">最大熵模型</h2>
<p>最大熵就是这样一个朴素的道理：</p>
<p>凡是我们知道的，就把它考虑进去，凡是不知道的，通通均匀分布！</p>
<h1 id="_19">应用</h1>
<h2 id="_20">最大似然估计</h2>
<p>似然函数取得最大值表示相应的参数能够使得统计模型最为合理。</p>
<p>最大似然估计的一般步骤：选取似然函数（一般是概率密度函数或概率质量函数），整理后求的最大值，实际应用中一般会取似然函数的对数作为求最值的函数，两者的结果是相同的。似然函数的最大值不一定唯一，也不一定存在。也矩发估计对比，最大似然估计的精度较高，信息损失较小，但是计算量大。</p>
<h1 id="_21">参考文献</h1>
<p><a href="https://wenku.baidu.com/view/c1a78c37b90d6c85ec3ac6b8.html">分布函数与概率密度</a></p>
<p><a href="http://m.blog.csdn.net/SmellyKitty/article/details/49130173">机器学习 先验概率、后验概率、贝叶斯公式、 似然函数</a><br />
<a href="http://blog.csdn.net/sunlylorn/article/details/19610589">似然函数Likelihood function</a><br />
<a href="http://blog.csdn.net/baimafujinji/article/details/51374202">先验概率、后验概率以及共轭先验</a></p>
<p><a href="https://www.jianshu.com/p/b570b1ba92bb">应该如何理解概率分布函数和概率密度函数？</a><br />
<a href=""></a></p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2018-01-05 00:00 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: '概率统计基本概念',
  owner: 'sthsf',
  repo: 'wiki',
  oauth: {
    client_id: '086c54c5fd95adfdc372',
    client_secret: '2ad9ebe87b952d2c77fccf99c334881b91eaa73d',
  },
  // ...
  // For more available options, check out the documentation below
})
gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

</div>
<div id="footer">
            <span>
                Copyright © 2019 LiYu.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/sthsf/wiki" target="_blank"> github </a>.
            </span>
</div>


<!--百度统计-->
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?90e1dcdd1938573c19f9ff6521188e91";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


</body>
</html>