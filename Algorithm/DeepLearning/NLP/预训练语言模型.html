<!DOCTYPE HTML>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <title>预训练语言模型 - LiYu's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Economics"/>
    <meta name="description" content="A wiki website of sthsf when I learned new knowledgy and technics."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-78529611-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>
<div id="container">
    
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm">Algorithm</a>&nbsp;»&nbsp;<a href="/wiki/#-DeepLearning">DeepLearning</a>&nbsp;»&nbsp;<a href="/wiki/#-NLP">NLP</a>&nbsp;»&nbsp;预训练语言模型</div>
</div>
<div class="clearfix"></div>
<div id="title">预训练语言模型</div>
<div id="content">
  <h1 id="_1">自然语言处理中的预训练过程</h1>
<h3 id="_2">预训练过程</h3>
<h3 id="_3">预训练模型</h3>
<h4 id="bert">Bert</h4>
<p>BERT （来⾃ Google）：作者 Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina<br />
Toutanova：BERT: Pre-training of Deep Bidirectional Transformers for Language<br />
Understanding（《BERT：⽤于语⾔理解的深度双向 Transformer 的预训练》）</p>
<h4 id="gpt">GPT</h4>
<p>GPT （来⾃ OpenAI）：作者 Alec Radford、Karthik Narasimhan、Tim Salimans 和 Ilya<br />
Sutskever：Improving Language Understanding by Generative Pre-Training （《通过⽣成式预训练<br />
提⾼语⾔理解能⼒》）</p>
<h4 id="gpt2">GPT2</h4>
<p>GPT （来⾃ OpenAI）：作者 Alec Radford、Karthik Narasimhan、Tim Salimans 和 Ilya<br />
Sutskever：Improving Language Understanding by Generative Pre-Training （《通过⽣成式预训练<br />
提⾼语⾔理解能⼒》）</p>
<h4 id="transformer-xl">Transformer-XL</h4>
<p>Transformer-XL （来⾃ Google/CMU）：作者 Zihang Dai、Zhilin Yang、Yiming Yang, Jaime<br />
Carbonell、Quoc V. Le、Ruslan Salakhutdinov：Transformer-XL: Attentive Language Models<br />
Beyond a Fixed-Length Context （《Transformer-XL：超⻓上下⽂关系的注意⼒语⾔模型》）</p>
<h4 id="xlnet">XLNet</h4>
<p>XLNet （来⾃ Google/CMU）：作者 Zihang Dai、Zhilin Yang、Yiming Yang、Jaime Carbonell、<br />
Quoc V. Le、Ruslan Salakhutdinov：XLNet: Generalized Autoregressive Pretraining for Language<br />
Understanding （《XLNet：⽤于语⾔理解的⼴义⾃回归预训练》）</p>
<h4 id="xlm">XLM</h4>
<p>XLM （来⾃ Facebook）：作者 Guillaume Lample 和 Alexis Conneau：Cross-lingual Language<br />
Model Pretraining （《跨语⾔的语⾔模型预训练》）</p>
<h1 id="_4">参考文献</h1>
<p><a href="https://www.leiphone.com/news/201803/yPQ7a4XMY7En4HBQ.html?ulu-rcmd=0_5021df_hot_3_5e51c35be24c4967abf6c0eaec9dc253">媲美人类有何不可？深度解读微软新 AI 翻译系统四大秘技</a><br />
<a href="https://chunml.github.io/ChunML.github.io/project/Sequence-To-Sequence/">Creating A Language Translation Model Using Sequence To Sequence Learning Approach</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/68446772">Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/66676144">BERT时代与后时代的NLP</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76912493">nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)</a></p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2019-06-02 00:00 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: '预训练语言模型',
  owner: 'sthsf',
  repo: 'wiki',
  oauth: {
    client_id: '086c54c5fd95adfdc372',
    client_secret: '2ad9ebe87b952d2c77fccf99c334881b91eaa73d',
  },
  // ...
  // For more available options, check out the documentation below
})
gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

</div>
<div id="footer">
            <span>
                Copyright © 2022 LiYu.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/sthsf/wiki" target="_blank"> github </a>.
            </span>
</div>


<!--百度统计-->
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?90e1dcdd1938573c19f9ff6521188e91";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


</body>
</html>