<!DOCTYPE HTML>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow基础知识---损失函数详解 - LiYu's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Economics"/>
    <meta name="description" content="A wiki website of sthsf when I learned new knowledgy and technics."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-78529611-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>
<div id="container">
    
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm">Algorithm</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm-DeepLearning">DeepLearning</a>&nbsp;»&nbsp;<a href="/wiki/#Algorithm-DeepLearning-Tensorflow学习笔记">Tensorflow学习笔记</a>&nbsp;»&nbsp;Tensorflow基础知识---损失函数详解</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow基础知识---损失函数详解</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">写在前面</a></li>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#_2">目标函数、损失函数、代价函数之间的关系与区别</a></li>
<li><a href="#0-1">0-1损失函数和绝对值损失</a></li>
<li><a href="#log">log对数损失函数</a></li>
<li><a href="#_3">平方损失函数</a></li>
<li><a href="#_4">指数损失函数</a></li>
<li><a href="#hinge">Hinge损失函数</a></li>
<li><a href="#tensorflowloss-function">Tensorflow中的loss function实现</a><ul>
<li><a href="#cross_entropy">cross_entropy交叉熵</a></li>
<li><a href="#sparse_softmax_cross_entropy_with_logits">sparse_softmax_cross_entropy_with_logits</a><ul>
<li><a href="#demo">Demo</a></li>
</ul>
</li>
<li><a href="#sigmoid_cross_entropy_with_logits">sigmoid_cross_entropy_with_logits</a></li>
<li><a href="#weighted_cross_entropy_with_logits">weighted_cross_entropy_with_logits</a></li>
<li><a href="#log_softmax">log_softmax</a></li>
</ul>
</li>
<li><a href="#sampled-loss-functions">sampled loss functions</a><ul>
<li><a href="#nce_loss">nce_loss</a></li>
<li><a href="#sampled_softmax_loss">sampled_softmax_loss</a></li>
</ul>
</li>
<li><a href="#sequence-to-sequenceloss-function">sequence to sequence中的loss function</a><ul>
<li><a href="#sampled_softmax_loss_1">sampled_softmax_loss</a></li>
<li><a href="#legacy_seq2seq">legacy_seq2seq</a></li>
</ul>
</li>
<li><a href="#_5">参考文献</a></li>
</ul>
</div>
<h1 id="_1">写在前面</h1>
<p>本文先介绍loss function的基本概念，然后主要归纳一下tensorflow中的loss_function.</p>
<h1 id="loss-function">Loss function</h1>
<p>在机器学习中，loss function（损失函数）也称cost function（代价函数），是用来计算预测值和真实值的差距。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。</p>
<h1 id="_2">目标函数、损失函数、代价函数之间的关系与区别</h1>
<p>有的书本上将其混为一类, 认为三者指代同一类. 但是实际上他们有一些差别的<br />
- 损失函数(loss funciton): 单体<br />
- 代价函数(cost function): 总体<br />
- 目标函数(object function): 更泛化的术语</p>
<p>从整体上来讲, 损失函数是代价函数的一部分, 代价函数是目标函数其中的一种. 详细的讲, 损失函数是针对单个训练样本而言的, 也就是算一个样本的误差. 代价函数是定义在整个训练集上的, 就是所有训练样本误差总和的平均, 也就是损失函数总和的平均. 但是, 有没有平均对后面参数的求解没有什么影响.</p>
<p>stack上有一段解释:<br />
- <strong><em>Loss function</em></strong> is usually a function defined on a data point, prediction and label, and measures the penalty. For example:<br />
    - square loss <br />
    - hinge loss<br />
    - 0/1 loss<br />
- <strong><em>Cost function</em></strong> is usually more general. It might be a sum of loss functions over your training set plus some model complexity penalty (regularization). For example:<br />
    - Mean Squared Error<br />
    - SVM cost function<br />
- <strong><em>Objective function</em></strong> is the most general term for any function that you optimize during training. For example:<br />
    - MLE is a type of objective function (which you maximize)<br />
    - Divergence between classes can be an objective function but it is barely a cost function, unless you define something artificial, like 1-Divergence, and name it a cost<br />
    - <br />
通常机器学习的每个算法中都会有一个目标函数, 算法的求解过程是通过对这个目标函数优化的过程, 损失函数越好, 通常模型的性能越好, 不同的算法使用的损失函数不一样.</p>
<p>损失函数分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>, 经验风险损失函数指预测结果和实际结果的差别, 结构风险损失函数是指经验风险损失函数加上正则项, 通常表示如下:</p>
<p>设总有$(N)$个样本的样本集为$((X,Y)=(x_i, y_i))$,那么总的损失函数为</p>
<p>$$<br />
\theta^* = argmin \frac{1}{N}\sum_{i=1}^N L(y_i, f(x_i, \theta_i)) + \lambda\Theta(\theta)<br />
$$</p>
<p>其中, 前面的均值项表示经验风险函数, $L$表示损失函数, 后面的是正则化项(regularizer)或者惩罚项(penalty term), $(y_i,i\in[1, N])$为样本$i$的真实值，$(f(x_i), i\in[1, N])$为样本$(i)$的预测值， $(f())$为分类或者回归函数。$(\lambda)$为正则项超参数, 常用的正则化方法包括：L1正则和L2正则</p>
<p>常见的损失函数有Zero-one Loss（0-1损失），Perceptron Loss（感知损失），Hinge Loss（Hinge损失），Log Loss（Log损失），Cross Entropy（交叉熵），Square Loss（平方误差），Absolute Loss（绝对误差），Exponential Loss（指数误差）等</p>
<p>一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上的损失函数值最小，即使得经验风险函数(Empirical risk)最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk）.</p>
<h1 id="0-1">0-1损失函数和绝对值损失</h1>
<p>0-1损失函数是指, 预测值和目标值相等则为1, 否则为0:</p>
<p>$$<br />
L(Y, f(X)) = \begin{cases}<br />
1 &amp; Y=f(X) \<br />
0 &amp; Y\not=f(x)<br />
\end{cases}<br />
$$</p>
<p>感知机就是使用的这种损失函数, 但是相等这个条件太过于严格, 一般会选用一个阈值,即$(|Y-f(X)| &lt; T)$来替换.</p>
<p>$$<br />
L(Y, f(X)) = \begin{cases}<br />
1 &amp; |Y-f(X)| \leq T \<br />
0 &amp; |Y-f(X)| &gt;T<br />
\end{cases}<br />
$$</p>
<h1 id="log">log对数损失函数</h1>
<p>logistic回归的损失函数就是对数损失函数, 在logistic回归的推导中, 它假设样本服从波努力分布(0-1)分布, 然后求的该分布的似然函数, 接着用对数求极值. </p>
<p>logistic回归并没有求对数似然函数的最大值, 而是把极大化当作一个思想, 进而推导出他的风险函数为最小化的负的似然函数. 从损失函数的角度上, 他就成为了log损失函数</p>
<p>log损失函数的标准形式:</p>
<p>$$<br />
L(Y, f(X)) = -log(P(Y|X))<br />
$$</p>
<p>在极大似然估计中, 通常都是先取对数再求导, 再找极值点, 这样做方便计算极大似然估计, 损失函数$L(Y, P(Y|X))$是指样本X在标签Y的情况下, 使得概率P(Y|X)达到最大值(<strong>利用已知的样本分布, 找到最大概率导致这种分布的参数值</strong>)</p>
<h1 id="_3">平方损失函数</h1>
<p>最小二乘法是线性回归的一种方法, 他将回归问题转化为凸优化的问题, 最小二乘法的基本原则是, 最优拟合曲线应该使得所有点到回归直线的距离之和最小, 通常用欧式距离进行距离的度量.</p>
<p>平方损失函数为:</p>
<p>$$<br />
L(Y, f(X)) = \sum_N{(Y-f(X))^2}<br />
$$</p>
<h1 id="_4">指数损失函数</h1>
<p>AdaBoost就是以指数函数为损失函数的.<br />
损失函数的标准形式:</p>
<p>$$<br />
L(Y, f(X)) = exp(-Yf(X))<br />
$$</p>
<h1 id="hinge">Hinge损失函数</h1>
<p>Hinge loss用于最大间隔分类, 其中最具有代表性的就是SVM.<br />
Hinge函数的标准形式:</p>
<p>$$<br />
L(y) = max(0, 1-ty)<br />
$$</p>
<p>或者可以表示为:</p>
<p>$$<br />
L(Y, f(X)) = max(0, 1-Yf(X))<br />
$$<br />
其中, t为目标值(-1, +1), y是分类器输出的预测值, 并不直接是类标签, 其含义为, 当t和y的符号相同时(表示y预测正确) 并且|y|&gt;= 1时, hinge loss为o; 当t和y的符号相反时, hinge loss随着y的增大线性增大.</p>
<h1 id="tensorflowloss-function">Tensorflow中的loss function实现</h1>
<h2 id="cross_entropy"><code>cross_entropy</code>交叉熵</h2>
<p>交叉熵刻画的是两个概率分布之间的距离，是分类问题中使用比较广泛的损失函数之一。给定两个概率分布p和q，通过交叉熵计算的两个概率分布之间的距离为：<br />
$$<br />
H(X=x)=-\sum_x{p(x)logq(x)}<br />
$$<br />
我们通过softmax回归将神经网络前向传播得到的结果变成交叉熵要求的概率分布得分。<br />
Tensorflow中定义的交叉熵函数如下：</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="c"># pylint: disable=invalid-name</span>
                                      <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes softmax cross entropy between `logits` and `labels`.&quot;&quot;&quot;</span>
</pre></div>


<ul>
<li>logits: 神经网络的最后一层输出，如果有batch的话，它的大小为[batch_size, num_classes], 单样本的话大小就是num_classes</li>
<li>labels: 样本的实际标签，大小与logits相同。且必须采用labels=y_，logits=y的形式将参数传入。</li>
</ul>
<p>具体的执行流程大概分为两步，第一步首先是对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率，对于单样本而言，就是输出一个num_classes大小的向量$([Y_1, Y_2, Y_3, ....])$, 其中$(Y_1, Y_2, Y_3)$分别表示属于该类别的概率， softmax的公式为：</p>
<p>$$softmax(x)_i={{exp(x_i)}\over{\sum_jexp(x_j)}}$$</p>
<p>第二步是对softmax输出的向量$([Y_1, Y_2, Y_3,...])$和样本的时机标签做一个交叉熵，公式如下：</p>
<p>$$H_{y'}(y)=-\sum_i{y_i'}log(y_i)$$</p>
<p>其中$(y_i')$指代实际标签向量中的第i个值，$(y_i)$就是softmax的输出向量$([Y_1, Y_2, Y_3,...])$中的第i个元素的值。<br />
显而易见。预测$(y_i)$越准确，结果的值就越小（前面有负号），最后求一个平均，就得到我们想要的loss了</p>
<p><strong>这里需要注意的是，这个函数返回值不是一个数，而是一个向量，如果要求交叉熵，我们要在做一步tf.resuce_sum操作，就是对向量里面的所有元素求和, 最后就能得到$(H_{y'}(y))$,如果要求loss，则需要做一步tf.reduce_mean操作，对向量求均值.</strong></p>
<p><strong>warning：</strong></p>
<ul>
<li>Tenosrflow中集成的交叉熵操作是施加在未经过Softmax处理的logits上, 这个操作的输入logits是未经缩放的, 该操作内部会对logits使用Softmax操作。</li>
<li>参数labels，ligits必须有相同的shape,如:[batch_size, num_classes]和相同的类型, 如:[(float16, float32, float64)中的一种]。</li>
</ul>
<p>下面这段代码可以测试上面的理论：</p>
<div class="hlcode"><pre><span class="c"># coding=utf-8</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>  

<span class="c"># 神经网络的输出</span>
<span class="n">logits</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">]])</span>  
<span class="c"># 使用softmax的输出</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>  
<span class="c"># 正确的标签</span>
<span class="n">y_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]])</span>  
<span class="c"># 计算交叉熵  </span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)))</span>  
<span class="c"># 使用tf.nn.softmax_cross_entropy_with_logits()函数直接计算神经网络的输出结果的交叉熵。</span>
<span class="c"># 但是不能忘记使用tf.reduce_sum()!!!!</span>
<span class="n">cross_entropy2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_</span><span class="p">))</span> 

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">softmax</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">c_e</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
    <span class="n">c_e2</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;step1:softmax result=&quot;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;step2:cross_entropy result=&quot;</span><span class="p">,</span> <span class="n">c_e</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Function(softmax_cross_entropy_with_logits) result=&quot;</span><span class="p">,</span> <span class="n">c_e2</span><span class="p">)</span>
</pre></div>


<p>输出结果：</p>
<div class="hlcode"><pre><span class="n">step1</span><span class="o">:</span><span class="n">softmax</span> <span class="n">result</span><span class="o">=</span>  
<span class="o">[[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]</span>  
 <span class="o">[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]</span>  
 <span class="o">[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]]</span>  
<span class="n">step2</span><span class="o">:</span><span class="n">cross_entropy</span> <span class="n">result</span><span class="o">=</span>  
<span class="mf">1.22282</span>  
<span class="n">Function</span><span class="o">(</span><span class="n">softmax_cross_entropy_with_logits</span><span class="o">)</span> <span class="n">result</span><span class="o">=</span>  
<span class="mf">1.2228</span> 
</pre></div>


<p>其中tf.clip_by_calue()函数可将一个tensor的元素数值限制在指定的范围内，这样可以防止一些错误运算，起到数值检查的作用。<br />
从结果可以看出softmax_cross_entropy_with_logits()与我们个公式逻辑是相符合的，整个过程可以大概了解到softmax_cross_entropy_with_logits()的操作情况。</p>
<h2 id="sparse_softmax_cross_entropy_with_logits">sparse_softmax_cross_entropy_with_logits</h2>
<p>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="c"># pylint: disable=invalid-name</span>
                                             <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                                             <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                             <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Computes sparse softmax cross entropy between `logits` and `labels`.</span>
<span class="sd">  Args:</span>
<span class="sd">    _sentinel: Used to prevent positional parameters. Internal, do not use.</span>
<span class="sd">    labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of</span>
<span class="sd">      `labels` and result) and dtype `int32` or `int64`. Each entry in `labels`</span>
<span class="sd">      must be an index in `[0, num_classes)`. Other values will raise an</span>
<span class="sd">      exception when this op is run on CPU, and return `NaN` for corresponding</span>
<span class="sd">      loss and gradient rows on GPU.</span>
<span class="sd">    logits: Unscaled log probabilities of shape</span>
<span class="sd">      `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float32` or `float64`.</span>
<span class="sd">    name: A name for the operation (optional).</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `Tensor` of the same shape as `labels` and of the same type as `logits`</span>
<span class="sd">    with the softmax cross entropy loss.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If logits are scalars (need to have rank &gt;= 1) or if the rank</span>
<span class="sd">      of the labels is not equal to the rank of the labels minus one.</span>
<span class="sd">  &quot;&quot;&quot;</span>
</pre></div>


<p>该函数与tf.nn.softmax_cross_entropy_with_logits()函数十分相似，唯一的区别在于labels的shape，该函数的labels要求是排他性的即只有一个正确的类别，如果labels的每一行不需要进行one_hot表示，可以使用tf.nn.sparse_softmax_cross_entropy_with_logits()。</p>
<h3 id="demo">Demo</h3>
<p>下面的代码列举了sparse_softmax_cross_entropy_with_logits()和softmax_cross_entropy_with_logits()的输入输出。</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">cost_compute</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_inputs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
    <span class="c"># shape = [batch_size * num_steps, ]</span>
    <span class="c"># labels&#39;shape = [batch_size * num_steps, num_classes]</span>
    <span class="c"># logits&#39;shape = [shape = [batch_size * num_steps, num_classes]]</span>
    <span class="c"># 这里可以使用tf.nn.sparse_softmax_cross_entropy_with_logits()和tf.nn.softmax_cross_entropy_with_logits()两种方式来计算rnn</span>
    <span class="c"># 但要注意labels的shape。</span>
    <span class="c"># eg.1</span>
    <span class="c"># loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(target_inputs, [-1]),</span>
    <span class="c">#                                                       logits=logits, name=&#39;loss&#39;)</span>

    <span class="c"># eg.2</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">target_inputs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>  <span class="c"># [batch_size, seq_length, num_classes]</span>
    <span class="c"># 不能使用logit.get_shape(), 因为在定义logit时shape=[None, num_steps], 这里使用会报错</span>
    <span class="c"># y_reshaped = tf.reshape(targets, logits.get_shape())  # y_reshaped: [batch_size * seq_length, num_classes]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">]),</span>
                                                   <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;loss&#39;</span><span class="p">)</span>

    <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;cost&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>


<p>完整代码请参考<a href="https://github.com/STHSF/DeepNaturalLanguageProcessing/tree/develop/ChineseSegmentation/src">bi_lstm_advanced.py</a></p>
<h2 id="sigmoid_cross_entropy_with_logits">sigmoid_cross_entropy_with_logits</h2>
<p>tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)<br />
sigmoid_cross_entropy_with_logits是TensorFlow最早实现的交叉熵算法。这个函数的输入是logits和labels，logits就是神经网络模型中的 W * X矩阵，注意不需要经过sigmoid，而labels的shape和logits相同，就是正确的标签值，例如这个模型一次要判断100张图是否包含10种动物，这两个输入的shape都是[100, 10]。注释中还提到这10个分类之间是独立的、不要求是互斥，这种问题我们称为多目标（多标签）分类，例如判断图片中是否包含10种动物中的一种或几种，标签值可以包含多个1或0个1。</p>
<h2 id="weighted_cross_entropy_with_logits">weighted_cross_entropy_with_logits</h2>
<p>tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)  <br />
weighted_sigmoid_cross_entropy_with_logits是sigmoid_cross_entropy_with_logits的拓展版，多支持一个pos_weight参数，在传统基于sigmoid的交叉熵算法上，正样本算出的值乘以某个系数。</p>
<h2 id="log_softmax">log_softmax</h2>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>    
</pre></div>


<h1 id="sampled-loss-functions">sampled loss functions</h1>
<h2 id="nce_loss">nce_loss</h2>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> 
               <span class="n">biases</span><span class="p">,</span> 
               <span class="n">inputs</span><span class="p">,</span> 
               <span class="n">labels</span><span class="p">,</span> 
               <span class="n">num_sampled</span><span class="p">,</span>
               <span class="n">num_classes</span><span class="p">,</span> 
               <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
               <span class="n">sampled_values</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">remove_accidental_hits</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
               <span class="n">partition_strategy</span><span class="o">=</span><span class="err">’</span><span class="n">mod</span><span class="err">’</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="err">’</span><span class="n">nce_loss</span><span class="err">’</span><span class="p">)</span>
</pre></div>


<h2 id="sampled_softmax_loss">sampled_softmax_loss</h2>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span>
                          <span class="n">biases</span><span class="p">,</span>
                          <span class="n">inputs</span><span class="p">,</span> 
                          <span class="n">labels</span><span class="p">,</span> 
                          <span class="n">num_sampled</span><span class="p">,</span> 
                          <span class="n">num_classes</span><span class="p">,</span> 
                          <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                          <span class="n">sampled_values</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                          <span class="n">remove_accidental_hits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                          <span class="n">partition_strategy</span><span class="o">=</span><span class="err">’</span><span class="n">mod</span><span class="err">’</span><span class="p">,</span> 
                          <span class="n">name</span><span class="o">=</span><span class="err">’</span><span class="n">sampled_softmax_loss</span><span class="err">’</span><span class="p">)</span>
</pre></div>


<h1 id="sequence-to-sequenceloss-function">sequence to sequence中的loss function</h1>
<h2 id="sampled_softmax_loss_1">sampled_softmax_loss</h2>
<p>sequence_loss_by_example(logits, targets, weights)</p>
<h2 id="legacy_seq2seq">legacy_seq2seq</h2>
<p>tf.contrib.legacy_seq2seq.sequence_loss_by_example</p>
<h1 id="_5">参考文献</h1>
<p><a href="https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing/179027">Objective function, cost function, loss function: are they the same thing?</a></p>
<p><a href="http://blog.csdn.net/marsjhao/article/details/72630147">1</a><br />
<a href="http://blog.csdn.net/u012436149/article/details/52874718">tensorflow学习笔记（三）：损失函数</a><br />
<a href="http://blog.csdn.net/appleml/article/details/54017873">sequence_loss_by_example(logits, targets, weights）</a></p>
<p><a href="https://www.cnblogs.com/hejunlin1992/p/8158933.html">机器学习中常见的几种损失函数</a></p>
<p><a href="http://www.mamicode.com/info-detail-2346633.html">一文读懂机器学习常用损失函数</a></p>
<p><a href="http://www.itongji.cn/detail?type=1040">机器学习中的损失函数</a></p>
</div>
<div id="content-footer">created in <span class="create-date date"> 2017-07-06 11:40 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: 'Tensorflow基础知识---损失函数详解',
  owner: 'sthsf',
  repo: 'wiki',
  oauth: {
    client_id: '086c54c5fd95adfdc372',
    client_secret: '2ad9ebe87b952d2c77fccf99c334881b91eaa73d',
  },
  // ...
  // For more available options, check out the documentation below
})
gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

</div>
<div id="footer">
            <span>
                Copyright © 2020 LiYu.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/sthsf/wiki" target="_blank"> github </a>.
            </span>
</div>


<!--百度统计-->
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?90e1dcdd1938573c19f9ff6521188e91";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


</body>
</html>