<!DOCTYPE HTML>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow中的损失函数详解 - sthsf's personal knowledge wiki</title>
    <meta name="keywords" content="technology, machine learning, data mining, economics, accounting"/>
    <meta name="description" content="A wiki website of sthsf when I learned new knowledgy and technics."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width" />

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-78529611-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>

<body>
<div id="container">
    
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#deep learning">deep learning</a>&nbsp;»&nbsp;Tensorflow中的损失函数详解</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow中的损失函数详解</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">写在前面</a></li>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#tensorflowloss-function">Tensorflow中的loss function实现</a><ul>
<li><a href="#cross_entropy">cross_entropy交叉熵</a></li>
<li><a href="#tfnnsigmoid_cross_entropy_with_logitslogits-targets-namenone">tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)*</a></li>
<li><a href="#tfnnlog_softmaxlogits-namenone">tf.nn.log_softmax(logits, name=None)</a></li>
<li><a href="#tfnnsparse_softmax_cross_entropy_with_logitslogits-labels-namenone">tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</a></li>
<li><a href="#tfnnweighted_cross_entropy_with_logitslogits-targets-pos_weight-namenone">tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)</a></li>
</ul>
</li>
<li><a href="#sampled-loss-functions">sampled loss functions</a><ul>
<li><a href="#tfnnnce_lossweights-biases-inputs-labels-num_samplednum_classes-num_true1-sampled_valuesnoneremove_accidental_hitsfalse-partition_strategymodnamence_loss">tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled,num_classes, num_true=1, sampled_values=None,remove_accidental_hits=False, partition_strategy=’mod’,name=’nce_loss’)</a></li>
<li><a href="#tfnnsampled_softmax_lossweights-biases-inputs-labels-num_sampled-num_classes-num_true1-sampled_valuesnoneremove_accidental_hitstrue-partition_strategymod-namesampled_softmax_loss">tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None,remove_accidental_hits=True, partition_strategy=’mod’, name=’sampled_softmax_loss’)</a></li>
</ul>
</li>
<li><a href="#sequence-to-sequenceloss-function">sequence to sequence中的loss function</a><ul>
<li><a href="#sequence_loss_by_examplelogits-targets-weights">sequence_loss_by_example(logits, targets, weights)</a></li>
<li><a href="#tfcontriblegacy_seq2seqsequence_loss_by_example">tf.contrib.legacy_seq2seq.sequence_loss_by_example</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">写在前面</h1>
<p>本文先介绍loss function的基本概念，然后主要归纳一下tensorflow中的loss_function.</p>
<h1 id="loss-function">Loss function</h1>
<p>在机器学习中，loss function（损失函数）也称cost function（代价函数），是用来计算预测值和真实值的差距。然后以loss function的最小值作为目标函数进行反向传播迭代计算模型中的参数，这个让loss function的值不断变小的过程称为优化。</p>
<p>设总有$(N)$个样本的样本集为$((X,Y)=(x_i, y_i))$,那么总的损失函数为<br />
$$ L=\sum_{i=1}^{n}{l(y_i, f(x_i))} $$<br />
其中 $(y_i,i\in[1, N])$为样本$i$的真实值，$(f(x_i), i\in[1, N])$为样本$(i)$的预测值， $(f())$为分类或者回归函数。</p>
<p>常见的损失函数有Zero-one Loss（0-1损失），Perceptron Loss（感知损失），Hinge Loss（Hinge损失），Log Loss（Log损失），Cross Entropy（交叉熵），Square Loss（平方误差），Absolute Loss（绝对误差），Exponential Loss（指数误差）等</p>
<p>一般来说，对于分类或者回归模型进行评估时，需要使得模型在训练数据上似的损失函数值最小，即使得经验风险函数（Empirical risk）最小化，但是如果只考虑经验风险，容易出现过拟合，因此还需要考虑模型的泛化性，一般常用的方法就是在目标函数中加上正则项，有损失项（loss term）加上正则项（regularization term）构成结构风险（Structural risk），那么损失函数变为：<br />
$$<br />
L =\sum_{i=1}^{N}{l(y_i, f(x_i))} +\lambda R(w)<br />
$$<br />
其中$(\lambda)$为正则项超参数，常用的正则化方法包括：L1正则和L2正则</p>
<h1 id="tensorflowloss-function">Tensorflow中的loss function实现</h1>
<h2 id="cross_entropy"><code>cross_entropy</code>交叉熵</h2>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="c"># pylint: disable=invalid-name</span>
                                      <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                      <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes softmax cross entropy between `logits` and `labels`.&quot;&quot;&quot;</span>
</pre></div>


<ul>
<li>logits: 神经网络的最后一层输出，如果有batch的话，它的大小为[batch_size, num_classes], 单样本的话大小就是num_classes</li>
<li>labels: 样本的实际标签，大小与logits相同。</li>
</ul>
<p>具体的执行流程大概分为两步，第一步首先是对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率，对于单样本而言，就是输出一个num_classes大小的向量$([Y_1, Y_2, Y_3, ....])$, 其中$(Y_1, Y_2, Y_3)$分别表示属于该类别的概率， softmax的公式为：</p>
<p>$$softmax(x)_i={{exp(x_i)}\over{\sum_jexp(x_j)}}$$</p>
<p>第二步是对softmax输出的向量$([Y_1, Y_2, Y_3,...])$和样本的时机标签做一个交叉熵，公式如下：</p>
<p>$$H_{y'}(y)=-\sum_i{y_i'}log(y_i)$$</p>
<p>其中$(y_i')$指代实际标签向量中的第i个值，$(y_i)$就是softmax的输出向量$([Y_1, Y_2, Y_3,...])$中的第i个元素的值。<br />
显而易见。预测$(y_i)$越准确，结果的值就越小（前面有负号），最后求一个平均，就得到我们想要的loss了</p>
<p>这里需要注意的是，这个函数返回值不是一个数，而是一个向量，如果要求交叉熵，我们要在做一步tf.resuce_sum操作，就是对向量里面的所有元素求和，最后就能得到$(H_{y'}(y))$,如果要求loss，则需要做一步tf.reduce_mean操作，对向量求均值。<br />
下面这段代码可以测试上面的理论：</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>  

<span class="c">#our NN&#39;s output  </span>
<span class="n">logits</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">]])</span>  
<span class="c">#step1:do softmax  </span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>  
<span class="c">#true label  </span>
<span class="n">y_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]])</span>  
<span class="c">#step2:do cross_entropy  </span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>  
<span class="c">#do cross_entropy just one step  </span>
<span class="n">cross_entropy2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">y_</span><span class="p">))</span><span class="c">#dont forget tf.reduce_sum()!!  </span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>  
    <span class="n">softmax</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  
    <span class="n">c_e</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>  
    <span class="n">c_e2</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cross_entropy2</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;step1:softmax result=&quot;</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="n">softmax</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;step2:cross_entropy result=&quot;</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="n">c_e</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Function(softmax_cross_entropy_with_logits) result=&quot;</span><span class="p">)</span>  
    <span class="k">print</span><span class="p">(</span><span class="n">c_e2</span><span class="p">)</span> 
</pre></div>


<p>输出结果：</p>
<div class="hlcode"><pre><span class="n">step1</span><span class="o">:</span><span class="n">softmax</span> <span class="n">result</span><span class="o">=</span>  
<span class="o">[[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]</span>  
 <span class="o">[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]</span>  
 <span class="o">[</span> <span class="mf">0.09003057</span>  <span class="mf">0.24472848</span>  <span class="mf">0.66524094</span><span class="o">]]</span>  
<span class="n">step2</span><span class="o">:</span><span class="n">cross_entropy</span> <span class="n">result</span><span class="o">=</span>  
<span class="mf">1.22282</span>  
<span class="n">Function</span><span class="o">(</span><span class="n">softmax_cross_entropy_with_logits</span><span class="o">)</span> <span class="n">result</span><span class="o">=</span>  
<span class="mf">1.2228</span> 
</pre></div>


<p>从结果可以看出softmax_cross_entropy_with_logits()与我们个公式逻辑是相符合的，整个过程可以大概了解到softmax_cross_entropy_with_logits()的操作情况。</p>
<h2 id="tfnnsigmoid_cross_entropy_with_logitslogits-targets-namenone">tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None)*</h2>
<h2 id="tfnnlog_softmaxlogits-namenone">tf.nn.log_softmax(logits, name=None)</h2>
<h2 id="tfnnsparse_softmax_cross_entropy_with_logitslogits-labels-namenone">tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</h2>
<h2 id="tfnnweighted_cross_entropy_with_logitslogits-targets-pos_weight-namenone">tf.nn.weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)</h2>
<h1 id="sampled-loss-functions">sampled loss functions</h1>
<h2 id="tfnnnce_lossweights-biases-inputs-labels-num_samplednum_classes-num_true1-sampled_valuesnoneremove_accidental_hitsfalse-partition_strategymodnamence_loss">tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled,num_classes, num_true=1, sampled_values=None,remove_accidental_hits=False, partition_strategy=’mod’,name=’nce_loss’)</h2>
<h2 id="tfnnsampled_softmax_lossweights-biases-inputs-labels-num_sampled-num_classes-num_true1-sampled_valuesnoneremove_accidental_hitstrue-partition_strategymod-namesampled_softmax_loss">tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None,remove_accidental_hits=True, partition_strategy=’mod’, name=’sampled_softmax_loss’)</h2>
<h1 id="sequence-to-sequenceloss-function">sequence to sequence中的loss function</h1>
<h2 id="sequence_loss_by_examplelogits-targets-weights">sequence_loss_by_example(logits, targets, weights)</h2>
<h2 id="tfcontriblegacy_seq2seqsequence_loss_by_example">tf.contrib.legacy_seq2seq.sequence_loss_by_example</h2>
</div>
<div id="content-footer">created in <span class="create-date date"> 2017-07-06 11:40 </span></div>
<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  title: 'Tensorflow中的损失函数详解',
  owner: 'sthsf',
  repo: 'wiki',
  oauth: {
    client_id: '086c54c5fd95adfdc372',
    client_secret: '2ad9ebe87b952d2c77fccf99c334881b91eaa73d',
  },
  // ...
  // For more available options, check out the documentation below
})
gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

</div>
<div id="footer">
            <span>
                Copyright © 2017 sthsf.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
                Fork me in <a href="https://github.com/sthsf/wiki" target="_blank"> github </a>.
            </span>
</div>


<!--百度统计-->
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?90e1dcdd1938573c19f9ff6521188e91";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>


</body>
</html>