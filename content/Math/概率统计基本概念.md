---
title: "概率统计基本概念"
layout: page
date: 2018-01-05 00:00
---

# 写在前面

机器学习离不开概率统计和线性代数的基本知识，如果想透彻的理解某个机器学习算法必然绕不开各种数学公式，掌握一些基本概念能够让我们更快的理解和推导这些公式。在这里，我将我自己在学习过程中遇到的基本知识整理起来。
# 随机变量
```
如果微积分研究的是变量的数学, 那么概率论与数理统计研究的就是随机变量的数学.
                                                    ----贾俊平
```
要弄清这两个概念, 得先从随机变量说起, 随机变量分为离散型随机变量和连续型随机变量, 不赘述, 了解到随机变量之后我们再了解下面几个概念.

# 离散型随机变量的概率分布, 概率函数和分布函数
在理解概率分布函数和概率密度函数之前, 我们可以先了解一下概率分布和概率函数.

所谓概率函数, 就是用函数的形式来表达概率.
$$
P_i = P(x=a_i), (i=1,2,3,4.....)
$$
在这个函数里, 自变量$(x)$为随机变量的取值, 因变量$(P_i)$为自变量取某一值的概率.

概率分布即为概率的分布, 下面的表格被称为离散型随机变量的**概率分布**, 完整的表达应该是**离散型随机变量的值分布和值的概率分布列表**, 这样更容易理解, 
<div align="center">

X|$(x_1)$|$(x_2)$|.....|$(x_n)$|.....|
-|-|-|-|-|-|
P|$(P_1)$|$(P_2)$|.....|$(P_n)$|.....|

</div>

在理解离散型概率分布的时候, 尤其要注意一点, 就是随机变量X的取值一定是全部取值.

分布函数的完整名字是概率分布函数,我们先看下面一段定义:

设离散型随机变量$(X)$的分布律是$(P\{X=X_k\}=P_k, k=1,2,3,....)$

则,     $(F(x) = P(X<=x) = \sum_{X_k<x}P_k)$

由于$(F(x))$是X取$(<=x)$的诸值$x_k$的概率之和, 故又称$(F(x))$为累积概率函数.

从上面的定义可以看出, 概率分布函数就是概率函数的累加, 可以看出, 概率函数和概率分布函数都是用来描述概率的不同手段.

所以可以总结为:
## 随机变量的分布函数
设 $(X(w))$ 是一个随机变量, 称函数 $(F(x)=P(X<=x), -\infty<x<\infty)$ 为随机变量 $(X)$ 的分布函数
### 分布函数的性质:
- 任意$(a<b)$, 总有$(F(a)<=F(b))$. (单调非减性)
- $(F(x))$是一个右连续函数;
- 任意$(x\in R)$, 总有$(0<=F(x)<=1)$(有界性), 且$(\lim_{x\rightarrow-\infty}F(x)=0, \lim_{x\rightarrow\infty} F(x)=1)$

# 连续型随机变量的概率密度函数和概率分布函数
连续型随机变量也有对应的概率函数和概率分布函数, 不过对于连续型随机变量, 概率函数叫做**概率密度函数**,陈希孺老师所著的概率论和数理统计中描述了密度函数的解释, 另外, 还可以用下面的方式来描述.
## 概率密度函数
如存在非负可积函数$(f(x))$, 使随机变量X取值与任一区间$((a, b])$的概率可表示成:
$$
P(a<=X <= b)=F(b) - F(a) = \int_{a}^bf(x)dx
$$
则称X为连续型随机变量, $(f(x))$为X的概率密度函数, 简称概率密度函数或密度.

概率密度函数用公式表示就是一个定积分函数, 定积分在数学中是用来求面积的, 而在这里, 也可以把概率表示为面积, 只不过概率密度函数曲线下的面积之和为1.

### 概率密度函数的性质
- $(f(x)>=0)$
- $(\int_{-\infty}^{\infty}f(x)dx = 1)$

## 分布函数和概率密度函数的关系
- $(F(x) = \int_{-\infty}^{\infty}f(t)dt)$
- $(f(x) = F'(x))$
## 注意点
- 概率密度函数$(f(x))$在点a处的取值, 不等于事件$({X=a})$的概率. 但是该值越大, X在a点附近取值的概率越大.



# 先验概率、后验概率、条件概率
## 先验概率

在贝叶斯统计中，先验概率分布，即关于某个变量X的概率分布，是在获得某些信息或者依据前，对X的不确定性所进行的猜测，这是对**不确定性**(而不是随机性)赋予一个量化的数值的表征，这个量化的数值可以是一个参数，或者是一个潜在的变量。

先验概率是指可以通过以往的经验和分析得到的概率，仅仅通过主观上的经验估计，也就是事先根据已有的只是推断。它是一种预判概率，可以是基于历史数据的统计，可以是由背景常识得出，也可以是人的主观观点给出，通常都是单独时间概率。

**先验概率的分类：**
利用过去历史资料计算得到的先验概率，称为客观先验概率。
当前历史资料无从取得或者资料不全时，凭人们的主观经验来判断得到的先验概率，称为主观先验概率。


## 条件概率
一般指一个事件发生后另一个事件发生的概率，一般的形式为$(P(A|B))$表示B发生的条件下A发生的概率。


## 似然函数
似然函数也成似然，是一个关于统计模型参数的函数，也就是这个函数中自变量是统计模型的参数，对于观测结果X，在参数集合$(\theta)$上的似然，就是在给定这些参数值的基础上，观察到的结果的概率$(L(\theta)=P(x|\theta))$。也就是说，似然是关于参数的函数，在参数给定的条件下，对于观察到的X的值的分布。

#### 定义
给定输出x时，关于参数$(\theta)$的似然函数$(L(\theta|x))$(在数值上)等于给定参数$(\theta)$后变量X=x的概率：

$$L(\theta)=P(x|\theta)$$

解释：
对参数$(\theta)$的似然函数的求值，（在数值上）等于观测结果x在给定参数$(\theta)$下的条件概率，也就是x的后验概率。一般似然函数的值越大表明在结果X=x下，此参数$(\theta)$越合理。因此形式上，似然函数也是一种条件概率函数，但是我们关注的变量改变了，关注的是A取值为参数$(\theta)$的似然值。

似然函数在统计判断中有重大作用，如在最大似然估计和费雪信息之中的应用扽等。似然性和或然性以及概率的意思相近，都是指某种事件发生的可能性，但是在统计学中，这三个名词的含义又有明确的区分，。

概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果。

似然性则是用于在已知观测所得到的结果时，对有关事物的性质的参数进行估计。

## 贝叶斯定理
贝叶斯公式，用来描述两个条件概率（后验概率之间的关系），比如$(P(A|B))$和$(P(B|A))$。按照乘法法则:

$$(P(AB) = P(A)*P(B|A) = P(B)*P(A|B))$$   

因此，公式也可以改写为：
$$
P(A|B) = P(A)*P(B|A)/ P(B)
$$
其中， $(P(B))$为标准化常量

贝叶斯法则表述如下：
$$
P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^n{P(B|A_i)P(A_i)}}
$$
其中 $(A_1, A_2, ..., A_n)$为完备事件组，即$(\bigcup_{i=1}^n{A_i}=\omega)$,$(A_iA_j=\phi)$,$(P(A_i)>0)$


## 后验概率
后验概率可以通过贝叶斯定理，用先验概率和似然函数计算出来，下面的公式就是用先验概率乘上似然函数，接着进行归一化，得到不定量X在Y=y的条件下的密度，即后验概率密度。

## 最大熵模型
最大熵就是这样一个朴素的道理：

凡是我们知道的，就把它考虑进去，凡是不知道的，通通均匀分布！



# 应用
## 最大似然估计
似然函数取得最大值表示相应的参数能够使得统计模型最为合理。

最大似然估计的一般步骤：选取似然函数（一般是概率密度函数或概率质量函数），整理后求的最大值，实际应用中一般会取似然函数的对数作为求最值的函数，两者的结果是相同的。似然函数的最大值不一定唯一，也不一定存在。也矩发估计对比，最大似然估计的精度较高，信息损失较小，但是计算量大。


# 参考文献
[分布函数与概率密度](https://wenku.baidu.com/view/c1a78c37b90d6c85ec3ac6b8.html)

[机器学习 先验概率、后验概率、贝叶斯公式、 似然函数](http://m.blog.csdn.net/SmellyKitty/article/details/49130173)
[似然函数Likelihood function](http://blog.csdn.net/sunlylorn/article/details/19610589)
[先验概率、后验概率以及共轭先验](http://blog.csdn.net/baimafujinji/article/details/51374202)

[应该如何理解概率分布函数和概率密度函数？](https://www.jianshu.com/p/b570b1ba92bb)
[]()







