---
title: "概率统计基本概念"
layout: page
date: 2018-01-05 00:00
---

# 写在前面
机器学习离不开概率统计和线性代数的基本知识，如果想透彻的理解某个机器学习算法必然绕不开各种数学公式，掌握一些基本概念能够让我们更快的理解和推导这些公式。在这里，我将我自己在学习过程中遇到的基本知识整理起来。

# 先验概率、后验概率、条件概率
## 先验概率

在贝叶斯统计中，先验概率分布，即关于某个变量X的概率分布，是在获得某些信息或者依据前，对X的不确定性所进行的猜测，这是对**不确定性**(而不是随机性)赋予一个量化的数值的表征，这个量化的数值可以是一个参数，或者是一个潜在的变量。

先验概率是指可以通过以往的经验和分析得到的概率，仅仅通过主观上的经验估计，也就是事先根据已有的只是推断。它是一种预判概率，可以是基于历史数据的统计，可以是由背景常识得出，也可以是人的主观观点给出，通常都是单独时间概率。

**先验概率的分类：**
利用过去历史资料计算得到的先验概率，称为客观先验概率。
当前历史资料无从取得或者资料不全时，凭人们的主观经验来判断得到的先验概率，称为主观先验概率。


## 条件概率
一般指一个事件发生后另一个事件发生的概率，一般的形式为$(P(A|B))$表示B发生的条件下A发生的概率。


## 似然函数
似然函数也成似然，是一个关于统计模型参数的函数，也就是这个函数中自变量是统计模型的参数，对于观测结果X，在参数集合$(\theta)$上的似然，就是在给定这些参数值的基础上，观察到的结果的概率$(L(\theta)=P(x|\theta))$。也就是说，似然是关于参数的函数，在参数给定的条件下，对于观察到的X的值的分布。

### 定义
给定输出x时，关于参数$(\theta)$的似然函数$(L(\theta|x))$(在数值上)等于给定参数$(\theta)$后变量X=x的概率：

$$L(\theta)=P(x|\theta)$$

解释：
对参数$(\theta)$的似然函数的求值，（在数值上）等于观测结果x在给定参数$(\theta)$下的条件概率，也就是x的后验概率。一般似然函数的值越大表明在结果X=x下，此参数$(\theta)$越合理。因此形式上，似然函数也是一种条件概率函数，但是我们关注的变量改变了，关注的是A取值为参数$(\theta)$的似然值。


## 贝叶斯定理
贝叶斯公式，用来描述两个条件概率（后验概率之间的关系），比如$(P(A|B))$和$(P(B|A))$。按照乘法法则：
$$
P(AB) = P(A)*P(B|A) = P(B)* P(A|B)
$$
因此，公式也可以改写为：
$$
P(A|B) = P(A)*P(B|A)/ P(B)       P(B)为标准化常量
$$

贝叶斯法则表述如下：
$$
P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{i=1}^n{P(B|A_i)P(A_i)}}
$$
其中 $(A_1, A_2, ..., A_n)$为完备事件组，即$(\bigcup_{i=1}^n{A_i}=\omiga)$





## 后验概率
后验概率可以通过贝叶斯定理，用先验概率和似然函数计算出来，下面的公式就是用先验概率乘上似然函数，接着进行归一化，得到不定量X在Y=y的条件下的密度，即后验概率密度。






