---
title: "集成学习"
layout: page
date: 2018-07-18 11:00
---

集成学习根据各个弱分类器之间有无依赖关系，分为Boosting和Bagging两大流派：

- Boosting流派，各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting DecisionTree)、Xgboost。

- Bagging流派，各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）。

## GBDT, lightGBM和XGBoost的区别
-  传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数；
- 传统GBDT以CART作为基分类器，xgboost支持线性分类器(线性问题就相当于用了L1正则化，L2正则化，L0不是凸函数，且不是连续的函数，不好优化)；
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度(使Θ更多的变为0，使我维度减下，所以，降低了复杂度)，xgboost在进行完一次迭代后，会乘以一个系数，也就是shrinkage，每次更新值缩减后，再进行更新。








# 参考文献
[Adaboost 算法](https://www.jianshu.com/p/389d28f853c0)
[浅谈 GBDT](https://www.jianshu.com/p/d55f7aaac4a7)
[史上最详细的XGBoost实战](https://blog.csdn.net/u013709270/article/details/78156207?locationNum=6&fps=1)
[机器学习---xgboost与lightgbm效果比较（2）](https://blog.csdn.net/zhouwenyuan1015/article/details/77481184)
[常用的集成学习方法](http://blog.51cto.com/yixianwei/2116117)
[机器学习（八）——集成学习](https://www.cnblogs.com/Rxma1805/p/8509498.html)
[集成学习基本原理：Adaboost，Bagging和Stacking](https://blog.csdn.net/leonliu1995/article/details/78848249)
[TensorFlow 教程 #05 - 集成学习](https://zhuanlan.zhihu.com/p/26943434)
[模型融合方法总结](https://blog.csdn.net/hust_tsb/article/details/76577599)
[模型融合的主要方法](https://blog.csdn.net/christ1750/article/details/51098880)
[Ensemble Learning-模型融合-Python实现](https://blog.csdn.net/shine19930820/article/details/75209021)
[关于多分类器融合算法](https://blog.csdn.net/frona_lee/article/details/8128924)
[集成学习算法(Ensemble Method)浅析](https://zhuanlan.zhihu.com/p/32798104)
[集成学习（ensemble learning ）该如何入门？](https://www.zhihu.com/question/29036379)
[集成学习总结 & Stacking方法详解](https://blog.csdn.net/willduan1/article/details/73618677/)
[Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)
[Stacking Learning在分类问题中的使用](https://blog.csdn.net/MrLevo520/article/details/78161590?locationNum=4&fps=1)
[]()
[]()
[]()
[]()
[]()
[]()
[]()
[]()
[]()

