---
title: "支持向量机总结"
layout: page
date: 2017-07-26 10:00
---
# 原理
支持向量机(Support Vecor Machine,SVM)是既可以解决线性问题又可以解决非线性问题，既可以用于分类，又可以用于回归的经典算法。它的基本模型是在特征空间中寻找间隔最大化的分离超平面，使距离最近的样本点到该超平面的距离尽可能的远。（间隔最大使它有别于感知机）

- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即Hard-Margin SVM；
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即Soft-Margin SVM；
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。


### SVM为什么采用间隔最大化？

- 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。SVM利用间隔最大化求得最优分离超平面，解是唯一的。
- 间隔最大化的超平面所产生的分类结果鲁棒性最好，对未知样本的泛化能力最强。


### 为什么要将求解SVM的原始问题转换为其对偶问题？

- 对偶问题往往更易求解。寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。
- 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
- 方便核函数的引入，进而推广到非线性分类问题。
- 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。
- 求解更高效，因为只用求解alpha系数，而alpha只有支持向量才非0，其它全部为0。


## SVM算法优缺点总结：

#### SVM算法的主要优点有：
解决高维特征的分类回归问题很有效,在特征维度大于样本数时依然有很好的效果。
仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。
使用核函数可以灵活的来解决各种非线性的分类回归问题。
样本量不是海量数据的时候，分类准确率高，泛化能力强。
#### SVM算法的主要缺点有：
特征维度远大于样本数时，SVM表现一般。
SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。
非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。
SVM对缺失数据敏感。
SVM要进行距离计算，需要对数据进行标准化处理，而决策树不需要。



## SVM和LR的异同点

#### SVM和LR的相同点：
- LR和SVM都是有监督的学习
- LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
- 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 
#### SVM和LR的不同点：
- 样本点对模型的作用不同。SVM只有关键的样本点（支持向量）对模型结果有影响，而LR每一个样本点都对模型有影响。
- 损失函数不同。SVM是hinge损失函数，LR是log损失函数。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
- 理论基础不同。SVM基于严格的数学推导，LR基于统计，可解释性比SVM好。
- 输出不同。LR可以对每个样本点给出类别判断的概率值，SVM无法做到。
- 可处理的特征空间维度不同。LR在特征空间维度很高时，表现较差。SVM则可以通过对偶求解高效应对这一挑战。
- 防过拟合能力不同。SVM模型中内含了L2正则，可有效防止过拟合。LR要自己添加正则项。
- 处理非线性分类问题能力不同。SVM可通过核函数灵活地将非线性问题转化为线性分类问题。LR需要手动进行特征转换。
- 计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高。
- 能力范围不同。 SVM拓展后，可解决回归问题，LR不能。
- 抗噪声数据能力不同。SVM的损失函数基于距离测度，抗噪声能力要强于LR。


# 参考文献
[支持向量机总结](https://blog.csdn.net/Yasin0/article/details/85799382)